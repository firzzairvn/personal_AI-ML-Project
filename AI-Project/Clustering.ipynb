{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Clustering data using scikit-learn\n\nClustering algorithms allow you to automatically find ways to group multidimentional data into clusters.\n\nIn this notebook, we'll use scikit-learn to predict clusters. \nscikit-learn provides implementations of many clustering algorithms.\nAs we experiment with each clustering algorithm, you'll see some of the advantages and drawbacks of each one.\nThe following algorithms are demonstrated in this notebook:\n\n* k-means\n* Mean shift\n* DBSCAN\n* Agglomerative clustering\n\nTo help visualize what we are doing, we'll use 3D charts to show how the clustering looks (with 3 selected dimensions) with two of the most popular Python visualization tools:\n\n* matplotlib enhanced with seaborn styling and animation\n* Plotly for a more interactive view (e.g., zoom and rotate a 3D chart with your mouse)\n\nFinally, because we are playing with the same data that we use for supervised learning, we will look at how our unsupervised creation of clusters might create customer groups that have similar attributes. That is, will our customer groups happen to indicate a likely risk of churn. Please keep in mind that the point of this exercise is not to create a churn classifier, we are just using that to see how these customer clusters might be interesting.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## Setup\n\n### Install python modules\n\n> NOTE!  Some pip installs require a kernel restart.\n\nThe shell command `pip install` is used to install Python modules. Some installs require a kernel restart to complete.\nTo avoid confusing errors, run the following cell once and then use the **Kernel** menu to restart the kernel before proceeding.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install --user plotly==4.1.1\n!pip install --user scikit-learn==0.21.3"}, {"cell_type": "markdown", "metadata": {}, "source": "## Imports\n\nImport the python modules that we need in the rest of the notebook."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%matplotlib inline\nfrom time import sleep\nfrom IPython.display import display, clear_output\nimport matplotlib as mpl\nimport matplotlib.animation\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nfrom matplotlib import lines\nfrom matplotlib.colors import ListedColormap\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.cluster import estimate_bandwidth\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons"}, {"cell_type": "markdown", "metadata": {}, "source": "## Load the customer and trading activity data\n\nUse the cell below to load the merged_customers.csv file into a pandas DataFrame."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": "df = pd.read_csv(\"https://raw.githubusercontent.com/IBM/ml-learning-path-assets/master/data/mergedcustomers.csv\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare the customer data\n\nRemove the CHURNRISK column. We don't want to use that label for clustering, but we will save it to try some external evaluation of our clusters later.\nSelect just a few numeric columns as features for our customer clusterer demos -- including gains minus losses combined into a single PROFIT_YTD.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Save the CHURNRISK label column for later.\nknown_risk = df['CHURNRISK']\n\n# Combine gains - losses into a profit column,\n# and select a some of the numeric trader data to use for our customer clustering example.\nkeep_columns = ['AGE', 'TOTALUNITSTRADED', 'DAYSSINCELASTTRADE', 'DAYSSINCELASTLOGIN', 'PROFIT_YTD']\n\ndf_churn = df.assign(PROFIT_YTD=df.apply(lambda row: row.NETREALIZEDGAINS_YTD - row.NETREALIZEDLOSSES_YTD, axis=1).values)[keep_columns]\n\n# Pick 3 features to use later in 3D charts\nx_name = 'AGE'\ny_name = 'PROFIT_YTD'\nz_name = 'DAYSSINCELASTTRADE'\n\ndf_churn.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare the scikit-learn datasets\n\nUsing the customer data is not the most obvious clustering experiment.\nscikit-learn make_blobs() and make_moons() will give us some better examples for certain algorithms.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Generate 10 features even though we generally only chart 3 (these are nice multidemensional, spherical clusters)\nn_features = 10\nn_samples = 1000\nrandom_state = 100\n\n# blobs is our generated data set.  blob_labels is the expected blob cluster membership.\nblobs, blob_labels = make_blobs(n_samples=n_samples, n_features=n_features, random_state=random_state)\n\n# Put the data set in a DataFrame and give it some columns names.\nblobs_df = pd.DataFrame(blobs, columns=['X', 'Y', 'Z', 'A', 'B', 'C', 'D', 'E', 'F', 'G'])\nblobs_df['CLUSTER'] = blob_labels\n\n# 2-dimensional moons dataset to show where some algorithms excel.\nmoons, moon_labels = make_moons(n_samples=n_samples, noise=.05)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Setup visualization\n\nConfigure seaborn to enhance our matplotlib charts."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "sns.set(style='darkgrid')\nplt.rcParams['figure.figsize'] = [8, 3]"}, {"cell_type": "markdown", "metadata": {}, "source": "Define some functions that will be used repeatedly for visualization."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 3D matplotlib (plus seaborn) charting with some data prep and optional center points\ndef show_scatter_3d(df, x_name, y_name, z_name, predicted=None, centers=None,\n                    marker='o', cmap=None, edgecolors=None, alpha=0.3,\n                    elev=25, azim=10, show_colorbar=True,\n                    xlim3d=None, ylim3d=None, zlim3d=None):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    \n    x_index = df.columns.get_loc(x_name)\n    y_index = df.columns.get_loc(y_name)\n    z_index = df.columns.get_loc(z_name)\n    \n    x = df[x_name]\n    y = df[y_name]\n    z = df[z_name]\n\n    if centers is not None:\n        spot_size=15  # Smaller spots make X more visible\n        for center in centers:\n            if center is not None:\n                ax.scatter(center[x_index], center[y_index], center[z_index], marker=\"X\", s=500, color='red')\n    else:\n        spot_size=30\n\n    # Pass in cmap if necessary, else get a right-sized list here\n    if not cmap:\n        cmap = ListedColormap(sns.color_palette(\"Set2\",len(set(predicted))))\n    \n    chart = ax.scatter(x, y, z, c=predicted, marker=marker, edgecolors=edgecolors, cmap=cmap, s=spot_size, alpha=alpha)\n    \n    # Add axis labels\n    ax.set_xlabel(x_name)\n    ax.set_ylabel(y_name)\n    ax.set_zlabel(z_name)\n    \n    # Optionally, set the axis limits:\n    if xlim3d:\n        ax.set_xlim3d(xlim3d)\n    if ylim3d:\n        ax.set_ylim3d(ylim3d)\n    if zlim3d:\n        ax.set_zlim3d(zlim3d)\n\n    # Make room for axis titles\n    plt.subplots_adjust(bottom=1, top=3, left=0, right=2)\n    \n    # Chart rotation\n    ax.view_init(elev, azim)\n    \n    if show_colorbar:\n        fig.colorbar(chart, ticks=list(set(predicted)))\n    \n    return fig, ax  "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "RdYlBu = plt.get_cmap('RdYlBu')  # colormap for moons\n\ndef show_scatter_moons(data, prediction, centers=[]):\n    plt.scatter(data[:, 0], data[:, 1], c=prediction, cmap=RdYlBu, alpha=0.5);\n    for center in centers:\n        plt.scatter(center[0], center[1], marker=\"X\", s=300, color='red')        "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Plotly 3D scatter chart is almost a one-liner, but use this function to keep the params in one place\ndef plotly_scatter_3d(df, x, y, z, color=None):\n    fig = px.scatter_3d(df, x=x, y=y, z=z, color=color,\n                    opacity=0.2, template='plotly_dark', color_continuous_scale=px.colors.qualitative.Set1)\n    fig.show()   "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Use a stacked bar chart for an external evaluation of the churn cluster vs known churn risk\n\n# Map the risk values to sortables (and still OK for the legend)\nrisk_map = {'High': '2: High', 'Medium': '1: Medium', 'Low': '0: Low'}\n    \n# Reusable chart to see if our clusters might help with churn risk prediction\ndef show_risk_by_cluster(data, risk):\n    \n    # Create DataFrame with predicted CLUSTER ID\n    data_df = pd.DataFrame(data=data, columns=['CLUSTER'])\n    \n    # Add CHURN_RISK using sortable values\n    data_df['CHURN_RISK'] = risk.map(risk_map)\n    \n    # Group by and count to get count of Hi/Med/Low in each cluster\n    grouped_data = data_df.groupby(['CLUSTER', 'CHURN_RISK']).size().to_frame().reset_index()\n    \n    # Pivot for charting\n    pivot = grouped_data.pivot(index='CLUSTER', columns='CHURN_RISK', values=0).fillna(0)\n    \n    # Sort to show descending High and then ascending Low counts\n    pivot = pivot.reindex(pivot.sort_values(by=['2: High', '0: Low'], ascending=[False, True]).index)\n\n    # Plot the sorted stacked bar chart\n    pivot.plot(kind='bar', stacked=True, color='gbr')\n\n    # Put the legend on the side    \n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.) "}, {"cell_type": "markdown", "metadata": {}, "source": "## Visualize the input data sets\n\nscikit-learn's make_blobs() generated hyperspheres and also returned a cluster membership (`y`).\nHere we can visualize (3 selected features of) the data and use color to show the cluster membership. \nLater, we'll ignore the cluster membership, but try to \"predict\" clusters using various algorthms."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "show_scatter_3d(blobs_df, 'X', 'Y', 'Z', predicted=blob_labels);"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# This is the same thing we just showed with matplotlib, but now we have tooltips and we can zoom and rotate.\n# Rotating the chart can be very helpful when clusters are overlapping in 3-dimensional space.\nplotly_scatter_3d(blobs_df, 'X', 'Y', 'Z', color='CLUSTER')"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": "# The moons data set is only two-dimensional but demonstrates some cluster algorithm differences.\n\nshow_scatter_moons(moons, moon_labels)"}, {"cell_type": "markdown", "metadata": {}, "source": "The customer data set is more like real world data. To visualize that we pick three features that we think are significant. We haven't predicted clusters yet, so we'll just show the color of the CHURNRISK label that we saved."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# We did not use k-means yet, let's just use the labels for color\nlabel_colors = known_risk.map({'High': 'r', 'Medium': 'b', 'Low': 'g'})\n\nshow_scatter_3d(df_churn, x_name, y_name, z_name, predicted=label_colors, show_colorbar=False);"}, {"cell_type": "markdown", "metadata": {}, "source": "## Unsupervised learning with clustering\n\nIn general, for each algorthm, we will:\n\n* Construct an instance with parameters and/or defaults for the clustering algorithm\n* Call fit_predict() to predict cluster membership for our dataset using that instance\n* Use charts to demonstrate the results\n* Evaluate the results\n\n### k-means\n\nFirst, we look at k-means clustering.\n\n* For k-means, you have to specify the number of clusters you want. We'll start with 3, but you can run this cell with different values to see what you get.\n* We'll add a red `x` to show centriods, when we can."}, {"cell_type": "markdown", "metadata": {}, "source": "### k-means with generated hyperspheres\n\nAlthough we used 10 dimensions and only show 3, the generated hyperspheres are easy for the k-means algorithm to cluster as expected if you pick the correct number for clusters.  If you change `n_clusters`, you'll see that the clusters are combined or split in ways that might not make sense visually."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "k = 3\nkmeans = KMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(blobs)\ncenters = kmeans.cluster_centers_\n\nshow_scatter_3d(blobs_df, 'X', 'Y', 'Z', predicted=y_pred, centers=centers);"}, {"cell_type": "markdown", "metadata": {}, "source": "### k-means with generated moons\n\nThe moons data, demonstrates a weakness of k-means. k-means does not recognize the dense crescent shapes which are easy to detect visually. k-means is limited to circles/spheres/hyperspheres. We choose n_clusters=2, but the result is probably not the best for this data set.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "k = 2\nkmeans = KMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(moons)\ncenters = kmeans.cluster_centers_\n    \nshow_scatter_moons(moons, y_pred, centers)"}, {"cell_type": "markdown", "metadata": {}, "source": "### k-means clustering with our customer data\n\nk-means cluster using our selected customer features looks OK visually. It certainly would be nice if the `n_clusters` was more automatic. We're using n_clusters=3 (and you can experiment).\n\nOne thing you might notice is that there are outlier data points or smaller clusters that don't look right. Adjusting n_clusters can help, but an algorithm that understands the concept of outliers would be more helpful.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "n_clusters = 3\nkmeans = KMeans(n_clusters=n_clusters)\npredicted = kmeans.fit_predict(df_churn.values)\ncenters = kmeans.cluster_centers_\n\nshow_scatter_3d(df_churn, x_name, y_name, z_name, predicted=predicted, centers=centers);"}, {"cell_type": "markdown", "metadata": {}, "source": "### External evaluation\n\nWe can do \"external evaluation\" by comparing the predicted clusters with the CHURNRISK label that we removed from the data.\nYou shouldn't expect unsupervised learning to be a good predictor of something so specific (supervised learning should work better),\nbut this demonstrates that these discovered customer groupings might have some things in common.\n\nWith n_clusters=3, you'll see that low risk customers were grouped pretty accurately. Another cluster is mostly high risk. The third group is high/medium.\nYou can run this again after predicting different numbers of clusters. Also, we'll see later how this compares to other algorithms."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "show_risk_by_cluster(predicted, known_risk)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Using Plotly\n\nIf you experimented above, you may have noticed that our k-means clusers were heavily influenced by the PROFIT_YTD column.\nUsing Plotly to rotate the chart and look at it from other angles can help you see that.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "X_df = df_churn.copy()\nX_df['CLUSTER'] = predicted\nplotly_scatter_3d(X_df, x_name, y_name, z_name, color='CLUSTER')"}, {"cell_type": "markdown", "metadata": {}, "source": "## Mean shift\n\nMean shift is centroid-based like k-means, but has some advantages:\n\n1. You don't need to specify n_clusters\n2. It is not limited to hyperspheres\n3. It recognizes density when seeking cluster centers\n4. `cluster_all=False` allows orphans\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "bandwidth = estimate_bandwidth(moons, quantile=.2, n_samples=1000) \nms = MeanShift(cluster_all=False, bandwidth=bandwidth)\ny_pred = ms.fit_predict(moons)\ncenters = ms.cluster_centers_\n    \nshow_scatter_moons(moons, y_pred, centers)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "ms = MeanShift(cluster_all=False)\npredicted = ms.fit_predict(df_churn.values)\nlabels = ms.labels_\ncenters = ms.cluster_centers_\n\nprint(\"Number of clusters: \", len(centers))"}, {"cell_type": "markdown", "metadata": {}, "source": "Trying mean shift on our customer data, the first advantage is obvious:\n\n* It automatically determined the number of clusters!\n\nSince we used `cluster_all=False`, you'll see that mean shift was able to orphan for some of our outlier points. You can experiment with that parameter to see how mean shift determines clusters when all points are clustered."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": "show_scatter_3d(df_churn, x_name, y_name, z_name, predicted=predicted, centers=centers,\n                show_colorbar=False,\n                cmap=ListedColormap(cm.Accent.colors[:6]));"}, {"cell_type": "markdown", "metadata": {}, "source": "With matplotlib animation we can cycle through charts that show the clusters and the orphans to help illustrate the separation accomplished by detecting sparse and dense regions."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "temp_df = df_churn.copy()\ntemp_df['CLUSTER'] = predicted\nno_outliers_df = temp_df[temp_df['CLUSTER']!=-1]\noutliers_df = temp_df[temp_df['CLUSTER']==-1]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "charts = [\n    {'dataframe': temp_df, 'centers': centers, 'cmap': ListedColormap(cm.Accent.colors[:6])},\n    {'dataframe': outliers_df, 'centers': centers, 'cmap': ListedColormap(cm.Accent.colors[:1])},\n    {'dataframe': temp_df, 'centers': centers, 'cmap': ListedColormap(cm.Accent.colors[:6])},\n    {'dataframe': no_outliers_df, 'centers': centers, 'cmap': ListedColormap(cm.Accent.colors[1:6])},\n    {'dataframe': temp_df, 'centers': centers, 'cmap': ListedColormap(cm.Accent.colors[:6])},\n    {'dataframe': no_outliers_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[1:6])},\n    {'dataframe': temp_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[:6])},\n    {'dataframe': no_outliers_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[1:6])},\n    {'dataframe': temp_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[:6])},\n    {'dataframe': outliers_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[:1])},\n    {'dataframe': temp_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[:6])},\n    {'dataframe': outliers_df, 'centers': None, 'cmap': ListedColormap(cm.Accent.colors[:1])}\n]\n\n# n loops for more time to watch charts\nloops = 2\nfor loop in range(loops):\n  for chart in charts:\n    clear_output(wait=True)\n    fig, ax = show_scatter_3d(chart['dataframe'], x_name, y_name, z_name, predicted=chart['dataframe']['CLUSTER'],\n                centers=chart['centers'],\n                show_colorbar=False, cmap=chart['cmap'],\n                elev=35, azim=10,\n                xlim3d=(10, 80), ylim3d=(-3000, 3000), zlim3d=(0, 20))\n        \n    display(fig)\n    plt.close(fig)\n    sleep(1)"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": "show_risk_by_cluster(predicted, known_risk)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nUnlike mean shift, DBSCAN understands the concept of outliers. It will not create mini-clusters of outliers, but instead will take all the outliers and put them in cluster -1. These outliers are sometimes called \"noise\".\n\nIn addition, since DBSCAN is density-based and not centroid-based, it is able to predict cluster shapes that k-means and mean shift cannot predict.\n\nFor DBSCAN the eps parameter is important. With our customer data, we use the `eps` paramater to specify the maximum distance between samples. By trying different values, you will see that it has a strong influence on the number of clusters and the amount of noise. This behavior makes it difficult to use with out customer data set, but with eps=110 we can demonstrate clusters with noise removed.\n\nA better example of how density-based clustering can recognize groups that centroid-based clustering cannot is our moons example.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from sklearn.cluster import DBSCAN\npredicted = DBSCAN(eps=110).fit_predict(df_churn)\n\nprint('Number of clusters:', len(set(predicted)) - (1 if -1 in predicted else 0))\nprint('Number of outliers:', list(predicted).count(-1))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "temp_df = df_churn.copy()\ntemp_df['CLUSTER'] = predicted\ntemp_df['KNOWN_RISK'] = known_risk\ntemp_df.columns = [str(x) for x in temp_df.columns.values]\nno_outliers_df = temp_df[temp_df['CLUSTER']!=-1]\noutliers_df = temp_df[temp_df['CLUSTER']==-1]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "show_scatter_3d(no_outliers_df, x_name, y_name, z_name, predicted=no_outliers_df['CLUSTER'], azim=2);"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "show_scatter_3d(outliers_df, x_name, y_name, z_name, predicted=outliers_df['CLUSTER'], cmap=cm.gist_stern, alpha=1, azim=5);"}, {"cell_type": "markdown", "metadata": {}, "source": "### DBSCAN with generated moons\n\nThe moons data clearly demonstrates how density-based clustering can predict group \"shapes\" that centroid-based clustering cannot.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "predicted = DBSCAN(eps=.1).fit_predict(moons)\n\nprint('Number of clusters:', len(set(predicted)) - (1 if -1 in predicted else 0))\nprint('Number of outliers:', list(predicted).count(-1))\n\nshow_scatter_moons(moons, predicted)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Hierarchical\n"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from sklearn.cluster import AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=None, distance_threshold=500, \n                             affinity='euclidean', linkage='complete')\npredicted = ac.fit_predict(df_churn.values)\n\nprint('Number of clusters:', len(set(predicted)))"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": "show_risk_by_cluster(predicted, known_risk)"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": "show_scatter_3d(df_churn, x_name, y_name, z_name, predicted=predicted);"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": "X_df = df_churn.copy()\nX_df['CLUSTER'] = predicted\nplotly_scatter_3d(X_df, x_name, y_name, z_name, color='CLUSTER')"}, {"cell_type": "markdown", "metadata": {}, "source": "<p><font size=-1 color=gray>\n&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n<p>\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\nexcept in compliance with the License. You may obtain a copy of the License at\nhttps://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software distributed under the\nLicense is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\nexpress or implied. See the License for the specific language governing permissions and\nlimitations under the License.\n</font></p>"}], "metadata": {"celltoolbar": "Raw Cell Format", "kernelspec": {"display_name": "Python 3.6", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2}